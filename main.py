from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from sklearn.neighbors import NearestNeighbors
import pandas as pd
import numpy as np
import os
import ast

# --- Load model embedding ---
model = SentenceTransformer('all-MiniLM-L6-v2')

# --- File l∆∞u d·ªØ li·ªáu ---
FILE_PATH = "data.csv"

# --- ƒê·ªçc d·ªØ li·ªáu t·ª´ CSV (n·∫øu c√≥) ---
if os.path.exists(FILE_PATH):
    df = pd.read_csv(FILE_PATH)
    # Chuy·ªÉn chu·ªói -> list l·∫°i cho c√°c c·ªôt ch·ª©a danh s√°ch
    for col in ["M√¥n h·ªçc", "Th·ªùi gian r·∫£nh"]:
        df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)
else:
    # N·∫øu ch∆∞a c√≥ file th√¨ t·∫°o d·ªØ li·ªáu m·∫´u
    data = {
        "T√™n": ["Ng·ªçc", "Lan", "Nam", "Vy", "B·∫£o"],
        "M√¥n h·ªçc": [["C∆° s·ªü l·∫≠p tr√¨nh"], ["To√°n r·ªùi r·∫°c"], ["K·ªπ nƒÉng m·ªÅm", "To√°n r·ªùi r·∫°c"],
                    ["Nh·∫≠p m√¥n CNTT"], ["K·ªπ nƒÉng m·ªÅm"]],
        "Th·ªùi gian r·∫£nh": [["S√°ng", "Chi·ªÅu"], ["Chi·ªÅu"], ["T·ªëi"], ["S√°ng"], ["S√°ng", "Chi·ªÅu"]],
        "Gi·ªõi t√≠nh": ["N·ªØ", "N·ªØ", "Nam", "N·ªØ", "Nam"],
        "S·ªü th√≠ch": [
            "Th√≠ch code web, ƒë·ªçc s√°ch c√¥ng ngh·ªá",
            "Y√™u th√≠ch To√°n h·ªçc v√† logic",
            "Th√≠ch l√†m vi·ªác nh√≥m, n√≥i chuy·ªán nhi·ªÅu",
            "Y√™u ngh·ªá thu·∫≠t, th√≠ch thi·∫øt k·∫ø",
            "Th√≠ch nghi√™n c·ª©u AI v√† c√¥ng ngh·ªá m·ªõi"
        ],
        "T√≠nh c√°ch": [
            "ƒêi·ªÅm tƒ©nh, ki√™n nh·∫´n",
            "NƒÉng ƒë·ªông, h∆∞·ªõng ngo·∫°i",
            "Vui v·∫ª, th√¢n thi·ªán",
            "Tr·∫ßm t√≠nh, s√°ng t·∫°o",
            "Ph√¢n t√≠ch logic, √≠t n√≥i"
        ]
    }
    df = pd.DataFrame(data)

# --- T√≠nh vector embedding ---
def compute_vectors(df):
    df["vector"] = df.apply(
        lambda row: model.encode(row["S·ªü th√≠ch"] + " " + row["T√≠nh c√°ch"]),
        axis=1
    )
    return df

df = compute_vectors(df)

# --- Ki·ªÉm tra ƒëi·ªÅu ki·ªán c·ª©ng ---
def valid_match(row, user_subjects, user_times, target_genders):
    subject_overlap = any(sub in row["M√¥n h·ªçc"] for sub in user_subjects)
    time_overlap = any(t in row["Th·ªùi gian r·∫£nh"] for t in user_times)
    gender_ok = (row["Gi·ªõi t√≠nh"] in target_genders) if target_genders else True
    return subject_overlap and time_overlap and gender_ok

# --- H√†m t√¨m b·∫°n h·ªçc ---
def find_best_matches_optimized(user_subjects, user_times, user_gender, target_genders,
                                user_hobby, user_personality, top_n=3):
    user_vector = model.encode(user_hobby + " " + user_personality).reshape(1, -1)
    valid_candidates = df[df.apply(
        lambda row: valid_match(row, user_subjects, user_times, target_genders),
        axis=1
    )].copy()

    if len(valid_candidates) == 0:
        return []

    X = np.array(list(valid_candidates["vector"]))
    X_norm = normalize(X, norm='l2')
    user_vec_norm = normalize(user_vector, norm='l2')

    knn = NearestNeighbors(
        n_neighbors=min(top_n, len(valid_candidates)),
        metric='euclidean',
        algorithm='ball_tree'
    )
    knn.fit(X_norm)
    distances, indices = knn.kneighbors(user_vec_norm)

    cosine_sim = (1 - (distances ** 2) / 2).clip(0, 1)
    top_matches = valid_candidates.iloc[indices[0]].copy()
    top_matches["ƒê·ªô h·ª£p (%)"] = (cosine_sim[0] * 100).round(2)

    return top_matches[["T√™n", "M√¥n h·ªçc", "Th·ªùi gian r·∫£nh", "Gi·ªõi t√≠nh",
                        "S·ªü th√≠ch", "T√≠nh c√°ch", "ƒê·ªô h·ª£p (%)"]]

# --- H√†m l∆∞u d·ªØ li·ªáu m·ªõi ---
def save_user_data(df, user_data):
    df = pd.concat([df, pd.DataFrame([user_data])], ignore_index=True)
    df.to_csv(FILE_PATH, index=False)
    return df

# --- Ch∆∞∆°ng tr√¨nh ch√≠nh ---
if __name__ == "__main__":
    print("=== üí¨ H·ªÜ TH·ªêNG GH√âP B·∫†N H·ªåC AI ===")

    user_name = input("Nh·∫≠p t√™n c·ªßa b·∫°n: ").strip()
    user_gender = input("Gi·ªõi t√≠nh (Nam/N·ªØ): ").strip()
    user_subjects = [s.strip() for s in input("M√¥n h·ªçc b·∫°n ƒëang h·ªçc (ngƒÉn c√°ch b·∫±ng d·∫•u ph·∫©y): ").split(",")]
    user_times = [t.strip() for t in input("Th·ªùi gian r·∫£nh (v√≠ d·ª•: S√°ng, Chi·ªÅu, T·ªëi): ").split(",")]
    target_gender = input("Mu·ªën t√¨m b·∫°n h·ªçc gi·ªõi t√≠nh n√†o (ƒë·ªÉ tr·ªëng n·∫øu kh√¥ng gi·ªõi h·∫°n): ").strip()
    user_hobby = input("S·ªü th√≠ch c·ªßa b·∫°n: ").strip()
    user_personality = input("T√≠nh c√°ch c·ªßa b·∫°n: ").strip()

    new_user = {
        "T√™n": user_name,
        "M√¥n h·ªçc": user_subjects,
        "Th·ªùi gian r·∫£nh": user_times,
        "Gi·ªõi t√≠nh": user_gender,
        "S·ªü th√≠ch": user_hobby,
        "T√≠nh c√°ch": user_personality
    }

    # --- L∆∞u d·ªØ li·ªáu m·ªõi ---
    df = save_user_data(df, new_user)
    print("\n‚úÖ D·ªØ li·ªáu c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng!")

    # --- T√≠nh vector cho ng∆∞·ªùi m·ªõi th√™m ---
    df = compute_vectors(df)

    # --- G·ª£i √Ω b·∫°n h·ªçc ph√π h·ª£p ---
    print("\nüîé ƒêang t√¨m b·∫°n h·ªçc ph√π h·ª£p nh·∫•t cho b·∫°n...\n")

    # B·ªè qua ch√≠nh ng∆∞·ªùi v·ª´a nh·∫≠p
    df_without_user = df[df["T√™n"] != user_name].copy()

    matches = find_best_matches_optimized(
        user_subjects, user_times, user_gender,
        [target_gender] if target_gender else [],
        user_hobby, user_personality
    )

    # N·∫øu ng∆∞·ªùi m·ªõi th√™m n·∫±m trong df, b·ªè qua
    if user_name in matches["T√™n"].values:
        matches = matches[matches["T√™n"] != user_name]

    if len(matches) == 0:
        print("‚ùå Kh√¥ng t√¨m th·∫•y b·∫°n h·ªçc ph√π h·ª£p.")
    else:
        print("‚úÖ G·ª£i √Ω b·∫°n h·ªçc ph√π h·ª£p nh·∫•t:\n")
        for i, row in matches.iterrows():
            print(f"- {row['T√™n']} ({row['Gi·ªõi t√≠nh']}) ‚Äî ƒê·ªô h·ª£p: {row['ƒê·ªô h·ª£p (%)']}%")
            print(f"  M√¥n h·ªçc: {', '.join(row['M√¥n h·ªçc'])}")
            print(f"  Th·ªùi gian r·∫£nh: {', '.join(row['Th·ªùi gian r·∫£nh'])}")
            print(f"  S·ªü th√≠ch: {row['S·ªü th√≠ch']}")
            print(f"  T√≠nh c√°ch: {row['T√≠nh c√°ch']}\n")
